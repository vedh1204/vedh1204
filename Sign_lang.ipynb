{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3ecb8c-8138-4b6b-8fed-2d4cdb63e1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bbca51f-ffdd-4b75-a57c-1b0872a4fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=2)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for handLms in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow(\"Hand Tracking\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc7b4a08-4552-48a0-b3e3-dce9da951f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned dataset created at: C:\\Users\\vedhr\\asl_cleaned\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Replace this with your actual dataset location\n",
    "src_root = r\"C:\\Users\\vedhr\\asl_alphabet_train\\asl_alphabet_train\"\n",
    "dst_root = r\"C:\\Users\\vedhr\\asl_cleaned\"\n",
    "\n",
    "os.makedirs(dst_root, exist_ok=True)\n",
    "\n",
    "for subdir in os.listdir(src_root):\n",
    "    label = ''.join([c for c in subdir if not c.isdigit()])  # A1 â†’ A\n",
    "    src_path = os.path.join(src_root, subdir)\n",
    "    dst_path = os.path.join(dst_root, label)\n",
    "    os.makedirs(dst_path, exist_ok=True)\n",
    "\n",
    "    for img in os.listdir(src_path):\n",
    "        src_img = os.path.join(src_path, img)\n",
    "        dst_img = os.path.join(dst_path, f\"{subdir}_{img}\")  # avoid filename conflicts\n",
    "        shutil.copyfile(src_img, dst_img)\n",
    "\n",
    "print(\"âœ… Cleaned dataset created at:\", dst_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e4e4d92-989a-4793-86f5-e6520637b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
      "Epoch [1/10] - Time: 392.91s - Train Loss: 1.0506 - Val Acc: 95.15%\n",
      "Epoch [2/10] - Time: 404.30s - Train Loss: 0.2530 - Val Acc: 98.63%\n",
      "Epoch [3/10] - Time: 396.43s - Train Loss: 0.1433 - Val Acc: 98.70%\n",
      "Epoch [4/10] - Time: 422.71s - Train Loss: 0.1032 - Val Acc: 99.66%\n",
      "Epoch [5/10] - Time: 423.47s - Train Loss: 0.0762 - Val Acc: 99.69%\n",
      "Epoch [6/10] - Time: 423.37s - Train Loss: 0.0661 - Val Acc: 99.87%\n",
      "Epoch [7/10] - Time: 424.10s - Train Loss: 0.0571 - Val Acc: 99.84%\n",
      "Epoch [8/10] - Time: 423.78s - Train Loss: 0.0465 - Val Acc: 99.84%\n",
      "Epoch [9/10] - Time: 428.05s - Train Loss: 0.0407 - Val Acc: 99.87%\n",
      "Epoch [10/10] - Time: 423.73s - Train Loss: 0.0383 - Val Acc: 99.78%\n",
      "\n",
      "âœ… Training complete in 4162.86 seconds (69.38 minutes)\n",
      "âœ… Model saved as 'asl_cnn_model.pth'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "data_dir = r\"C:\\Users\\vedhr\\asl_cleaned\"  # <- Point to cleaned folder (A/, B/, ..., space/)\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# =========================\n",
    "# Device setup\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# =========================\n",
    "# Data loading and transforms\n",
    "# =========================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "class_names = dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "\n",
    "# =========================\n",
    "# CNN Model Definition\n",
    "# =========================\n",
    "class ASLCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ASLCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 6 * 6, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "model = ASLCNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# =========================\n",
    "# Training Loop\n",
    "# =========================\n",
    "train_loss, val_loss, val_acc = [], [], []\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_loss.append(total_loss / len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss.append(total_val_loss / len(val_loader))\n",
    "    val_acc.append(correct / total)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Time: {epoch_time:.2f}s - Train Loss: {train_loss[-1]:.4f} - Val Acc: {val_acc[-1]*100:.2f}%\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\nâœ… Training complete in {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "# =========================\n",
    "# Save Model\n",
    "# =========================\n",
    "torch.save(model.state_dict(), \"asl_cnn_model.pth\")\n",
    "print(\"âœ… Model saved as 'asl_cnn_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d49babc-cd67-474e-9568-5b05bae4a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes loaded: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
      "ðŸ“¸ Webcam started â€” press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# ========== 1. Load the trained model ==========\n",
    "class ASLCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ASLCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 6 * 6, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 29)  # 26 alphabets + del + nothing + space\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Load model\n",
    "model = ASLCNN(num_classes=29)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"asl_cnn_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ========== 2. Load class labels ==========\n",
    "data_dir = r\"C:\\Users\\vedhr\\asl_cleaned\"  # Path to your class folders used in training\n",
    "class_names = sorted(os.listdir(data_dir))\n",
    "print(\"Classes loaded:\", class_names)\n",
    "\n",
    "# ========== 3. Define transform ==========\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# ========== 4. Initialize MediaPipe and OpenCV ==========\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"ðŸ“¸ Webcam started â€” press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"âŒ Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get bounding box from landmarks\n",
    "            x_coords = [lm.x for lm in hand_landmarks.landmark]\n",
    "            y_coords = [lm.y for lm in hand_landmarks.landmark]\n",
    "            x_min, x_max = int(min(x_coords) * w), int(max(x_coords) * w)\n",
    "            y_min, y_max = int(min(y_coords) * h), int(max(y_coords) * h)\n",
    "\n",
    "            margin = 20\n",
    "            x1, y1 = max(0, x_min - margin), max(0, y_min - margin)\n",
    "            x2, y2 = min(w, x_max + margin), min(h, y_max + margin)\n",
    "            hand_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "            if hand_img.size != 0:\n",
    "                try:\n",
    "                    # Convert to PIL image\n",
    "                    hand_rgb = cv2.cvtColor(hand_img, cv2.COLOR_BGR2RGB)\n",
    "                    hand_pil = Image.fromarray(hand_rgb)\n",
    "                    hand_tensor = transform(hand_pil).unsqueeze(0).to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(hand_tensor)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        label = class_names[predicted.item()]\n",
    "                    \n",
    "                    # Draw result\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 255, 0), 2)\n",
    "                    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                1, (0, 255, 0), 2)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"âš ï¸ Error during prediction:\", e)\n",
    "\n",
    "    cv2.imshow(\"ASL Sign Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd10e41-76f8-4354-9cc8-54a100748696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: F -- File: F_F1.jpg\n",
      "Predicted: F -- File: F_F10.jpg\n",
      "Predicted: F -- File: F_F100.jpg\n",
      "Predicted: F -- File: F_F1000.jpg\n",
      "Predicted: F -- File: F_F1001.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "# Pick sample test images from various classes\n",
    "test_images = glob.glob(r\"C:\\Users\\vedhr\\asl_cleaned\\F\\*.jpg\")[:5]\n",
    "\n",
    "for img_path in test_images:\n",
    "    image = Image.open(img_path)\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        print(f\"Predicted: {class_names[predicted.item()]} -- File: {os.path.basename(img_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be98b3e-dfbc-4b26-9e6e-136d5ca540fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA Device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Memory Allocated: 1180.01220703125 MB\n",
      "Memory Cached: 1552.0 MB\n",
      "Classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# âœ… FINAL FAST GPU-OPTIMIZED CODE\n",
    "# ================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ========= SYSTEM OPTIMIZATION CHECKLIST =========\n",
    "# [âœ”] Dataset on SSD (not HDD or OneDrive)\n",
    "# [âœ”] Use num_workers in DataLoader (4â€“8)\n",
    "# [âœ”] Use pin_memory=True in DataLoader\n",
    "# [âœ”] Use larger batch size (128)\n",
    "# [âœ”] Minimal transforms to reduce CPU load\n",
    "# ================================================\n",
    "\n",
    "# ========== DEVICE SETUP ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == 'cuda':\n",
    "    print(\"CUDA Device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Memory Allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
    "    print(\"Memory Cached:\", torch.cuda.memory_reserved() / 1024**2, \"MB\")\n",
    "\n",
    "# ========== DATA PATH ==========\n",
    "data_dir = r\"C:\\\\Users\\\\vedhr\\\\asl_cleaned\"\n",
    "\n",
    "# ========== TRANSFORMS ==========\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# ========== LOAD DATA ==========\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "class_names = dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# Split into 90% train / 10% val\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=128, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ========== CNN MODEL ==========\n",
    "class ASLCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ASLCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 6 * 6, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "model = ASLCNN(num_classes).to(device)\n",
    "\n",
    "# ========== TRAINING SETUP ==========\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 30\n",
    "\n",
    "# ========== TRAINING LOOP ==========\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Time: {end-start:.2f}s - Train Loss: {train_loss/len(train_loader):.4f} - Val Acc: {(correct/total)*100:.2f}%\")\n",
    "\n",
    "# ========== SAVE MODEL ==========\n",
    "torch.save(model.state_dict(), \"asl_cnn_model.pth\")\n",
    "print(\"âœ… Model saved as 'asl_cnn_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad03f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Torch GPU)",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
